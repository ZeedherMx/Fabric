"""
Code Generator Agent - Generates the actual chatbot code
"""
import os
import json
from typing import Dict, Any, List
from pathlib import Path
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage
from jinja2 import Environment, FileSystemLoader, Template
from ..core.models import ChatbotConfig
from ..core.config import settings
from ..core.observability import observability

class CodeGenerator:
    """Agent responsible for generating chatbot code"""
    
    def __init__(self):
        self.llm = ChatGroq(
            groq_api_key=settings.groq_api_key,
            model_name=settings.default_model,
            temperature=0.1  # Very low temperature for consistent code generation
        )
        self.templates_path = Path(settings.templates_path)
    
    @observability.track_llm_call
    def generate_chatbot_code(self, config: ChatbotConfig, architecture: Dict[str, Any], output_path: str) -> List[str]:
        """Generate complete chatbot code based on config and architecture"""
        
        generated_files = []
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate main application file
        main_file = self._generate_main_app(config, architecture, output_dir)
        generated_files.append(main_file)
        
        # Generate agent files
        if architecture["type"] == "multi_agent":
            agent_files = self._generate_multi_agent_system(config, architecture, output_dir)
            generated_files.extend(agent_files)
        else:
            agent_file = self._generate_single_agent(config, architecture, output_dir)
            generated_files.append(agent_file)
        
        # Generate UI components
        ui_files = self._generate_ui_components(config, output_dir)
        generated_files.extend(ui_files)
        
        # Generate configuration files
        config_files = self._generate_config_files(config, output_dir)
        generated_files.extend(config_files)
        
        # Generate Docker files
        if config.enable_docker:
            docker_files = self._generate_docker_files(config, output_dir)
            generated_files.extend(docker_files)
        
        # Generate requirements and setup files
        setup_files = self._generate_setup_files(config, architecture, output_dir)
        generated_files.extend(setup_files)
        
        return generated_files
    
    def _generate_main_app(self, config: ChatbotConfig, architecture: Dict[str, Any], output_dir: Path) -> str:
        """Generate the main application file"""
        
        template_content = '''"""
{{ config.name }} - AI Chatbot
Generated by Chatbot Factory
"""
import os
import asyncio
from typing import Dict, Any
import gradio as gr
from fastapi import FastAPI
from langchain_groq import ChatGroq
from langgraph import StateGraph, END
{% if config.enable_rag %}
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
{% endif %}
{% if observability_enabled %}
from langfuse import Langfuse
{% endif %}

# Configuration
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
MODEL_NAME = "{{ architecture.tech_stack.llm }}"

class {{ config.name.replace(' ', '').replace('-', '') }}Bot:
    """Main chatbot class"""
    
    def __init__(self):
        self.llm = ChatGroq(
            groq_api_key=GROQ_API_KEY,
            model_name="{{ settings.default_model }}",
            temperature={{ settings.temperature }}
        )
        {% if config.enable_rag %}
        self.setup_rag()
        {% endif %}
        {% if config.enable_memory %}
        self.conversation_memory = {}
        {% endif %}
        self.setup_graph()
    
    {% if config.enable_rag %}
    def setup_rag(self):
        """Setup RAG components"""
        self.embeddings = HuggingFaceEmbeddings()
        self.vectorstore = Chroma(
            persist_directory="./chroma_db",
            embedding_function=self.embeddings
        )
    {% endif %}
    
    def setup_graph(self):
        """Setup the conversation graph"""
        workflow = StateGraph(dict)
        
        # Add nodes
        workflow.add_node("process_input", self.process_input)
        workflow.add_node("generate_response", self.generate_response)
        
        # Add edges
        workflow.add_edge("process_input", "generate_response")
        workflow.add_edge("generate_response", END)
        
        # Set entry point
        workflow.set_entry_point("process_input")
        
        self.graph = workflow.compile()
    
    async def process_input(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Process user input"""
        user_message = state.get("user_message", "")
        user_id = state.get("user_id", "default")
        
        # Add context processing here
        {% if config.enable_rag %}
        # RAG processing
        relevant_docs = self.vectorstore.similarity_search(user_message, k=3)
        context = "\\n".join([doc.page_content for doc in relevant_docs])
        state["context"] = context
        {% endif %}
        
        {% if config.enable_memory %}
        # Memory processing
        if user_id not in self.conversation_memory:
            self.conversation_memory[user_id] = []
        
        self.conversation_memory[user_id].append({"role": "user", "content": user_message})
        state["conversation_history"] = self.conversation_memory[user_id][-10:]  # Last 10 messages
        {% endif %}
        
        return state
    
    async def generate_response(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Generate chatbot response"""
        user_message = state.get("user_message", "")
        context = state.get("context", "")
        conversation_history = state.get("conversation_history", [])
        
        # Build system prompt
        system_prompt = f"""You are {{ config.name }}, {{ config.description }}.
        
Personality traits: {{ config.personality_traits | join(', ') }}
Tone: {{ config.tone }}
Domain expertise: {{ config.domain_expertise | join(', ') }}

{% if config.enable_rag %}
Use the following context to inform your response:
{context}
{% endif %}

Be helpful, accurate, and maintain your personality throughout the conversation."""

        # Build conversation context
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(conversation_history)
        messages.append({"role": "user", "content": user_message})
        
        # Generate response
        response = await self.llm.ainvoke([
            {"role": msg["role"], "content": msg["content"]} for msg in messages
        ])
        
        bot_response = response.content
        
        {% if config.enable_memory %}
        # Update memory
        user_id = state.get("user_id", "default")
        self.conversation_memory[user_id].append({"role": "assistant", "content": bot_response})
        {% endif %}
        
        state["bot_response"] = bot_response
        return state
    
    async def chat(self, message: str, history: list, user_id: str = "default") -> str:
        """Main chat interface"""
        try:
            state = {
                "user_message": message,
                "user_id": user_id,
                "history": history
            }
            
            result = await self.graph.ainvoke(state)
            return result.get("bot_response", "I'm sorry, I couldn't process your request.")
            
        except Exception as e:
            return f"Error: {str(e)}"

# Initialize chatbot
chatbot = {{ config.name.replace(' ', '').replace('-', '') }}Bot()

# Gradio Interface
def chat_interface(message, history):
    """Gradio chat interface"""
    response = asyncio.run(chatbot.chat(message, history))
    return response

# Create Gradio app
demo = gr.ChatInterface(
    fn=chat_interface,
    title="{{ config.name }}",
    description="{{ config.description }}",
    theme="{{ config.ui_theme }}",
    {% if config.logo_url %}
    avatar_images=[None, "{{ config.logo_url }}"],
    {% endif %}
)

# FastAPI app for API access
app = FastAPI(title="{{ config.name }} API")

@app.post("/chat")
async def api_chat(message: str, user_id: str = "default"):
    """API endpoint for chat"""
    response = await chatbot.chat(message, [], user_id)
    return {"response": response}

if __name__ == "__main__":
    import uvicorn
    
    # Launch Gradio in a separate thread
    demo.queue().launch(
        server_name="0.0.0.0",
        server_port={{ config.port }},
        share=False
    )
'''
        
        template = Template(template_content)
        content = template.render(
            config=config,
            architecture=architecture,
            settings=settings,
            observability_enabled=bool(settings.opik_api_key or settings.langfuse_secret_key)
        )
        
        main_file = output_dir / "main.py"
        with open(main_file, "w") as f:
            f.write(content)
        
        return str(main_file)
    
    def _generate_single_agent(self, config: ChatbotConfig, architecture: Dict[str, Any], output_dir: Path) -> str:
        """Generate single agent implementation"""
        
        agent_content = '''"""
Single Agent Implementation for {{ config.name }}
"""
from typing import Dict, Any, List
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage
{% if config.enable_function_calling %}
from langchain_core.tools import tool
{% endif %}

class {{ config.name.replace(' ', '').replace('-', '') }}Agent:
    """Main agent for {{ config.name }}"""
    
    def __init__(self):
        self.llm = ChatGroq(
            groq_api_key=os.getenv("GROQ_API_KEY"),
            model_name="{{ settings.default_model }}",
            temperature={{ settings.temperature }}
        )
        {% if config.enable_function_calling %}
        self.tools = self._setup_tools()
        {% endif %}
    
    {% if config.enable_function_calling %}
    def _setup_tools(self):
        """Setup available tools"""
        tools = []
        
        {% for integration in config.integrations %}
        {% if integration.type == 'rest_api' %}
        @tool
        def api_call(endpoint: str, method: str = "GET", data: dict = None) -> str:
            """Make API calls to external services"""
            import requests
            try:
                if method.upper() == "GET":
                    response = requests.get(endpoint)
                elif method.upper() == "POST":
                    response = requests.post(endpoint, json=data)
                return response.json()
            except Exception as e:
                return f"API call failed: {str(e)}"
        
        tools.append(api_call)
        {% endif %}
        {% endfor %}
        
        return tools
    {% endif %}
    
    async def process(self, message: str, context: Dict[str, Any] = None) -> str:
        """Process user message and generate response"""
        
        system_prompt = f"""You are {{ config.name }}, {{ config.description }}.
        
Your personality: {{ config.personality_traits | join(', ') }}
Your expertise: {{ config.domain_expertise | join(', ') }}
Communication style: {{ config.tone }}

Always be helpful and stay in character."""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=message)
        ]
        
        {% if config.enable_function_calling %}
        # Bind tools to LLM
        llm_with_tools = self.llm.bind_tools(self.tools)
        response = await llm_with_tools.ainvoke(messages)
        {% else %}
        response = await self.llm.ainvoke(messages)
        {% endif %}
        
        return response.content
'''
        
        template = Template(agent_content)
        content = template.render(config=config, settings=settings)
        
        agent_file = output_dir / "agent.py"
        with open(agent_file, "w") as f:
            f.write(content)
        
        return str(agent_file)
    
    def _generate_multi_agent_system(self, config: ChatbotConfig, architecture: Dict[str, Any], output_dir: Path) -> List[str]:
        """Generate multi-agent system files"""
        generated_files = []
        
        # Create agents directory
        agents_dir = output_dir / "agents"
        agents_dir.mkdir(exist_ok=True)
        
        # Generate coordinator
        coordinator_content = '''"""
Coordinator Agent - Routes tasks between specialized agents
"""
from typing import Dict, Any, List
from langchain_groq import ChatGroq
from langgraph import StateGraph, END

class CoordinatorAgent:
    """Coordinates tasks between specialized agents"""
    
    def __init__(self, agents: Dict[str, Any]):
        self.llm = ChatGroq(
            groq_api_key=os.getenv("GROQ_API_KEY"),
            model_name="{{ settings.default_model }}",
            temperature=0.3
        )
        self.agents = agents
        self.setup_graph()
    
    def setup_graph(self):
        """Setup coordination graph"""
        workflow = StateGraph(dict)
        
        workflow.add_node("analyze_request", self.analyze_request)
        workflow.add_node("route_to_agent", self.route_to_agent)
        workflow.add_node("synthesize_response", self.synthesize_response)
        
        workflow.add_edge("analyze_request", "route_to_agent")
        workflow.add_edge("route_to_agent", "synthesize_response")
        workflow.add_edge("synthesize_response", END)
        
        workflow.set_entry_point("analyze_request")
        self.graph = workflow.compile()
    
    async def analyze_request(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze user request to determine routing"""
        user_message = state.get("user_message", "")
        
        analysis_prompt = f"""Analyze this user request and determine which specialized agent should handle it:
        
Available agents: {list(self.agents.keys())}
User request: {user_message}

Respond with just the agent name that should handle this request."""

        response = await self.llm.ainvoke([{"role": "user", "content": analysis_prompt}])
        
        selected_agent = response.content.strip().lower()
        state["selected_agent"] = selected_agent
        return state
    
    async def route_to_agent(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Route request to selected agent"""
        selected_agent = state.get("selected_agent", "")
        user_message = state.get("user_message", "")
        
        if selected_agent in self.agents:
            agent_response = await self.agents[selected_agent].process(user_message)
            state["agent_response"] = agent_response
        else:
            state["agent_response"] = "I'm not sure how to handle that request."
        
        return state
    
    async def synthesize_response(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """Synthesize final response"""
        agent_response = state.get("agent_response", "")
        state["final_response"] = agent_response
        return state
    
    async def coordinate(self, message: str) -> str:
        """Main coordination method"""
        state = {"user_message": message}
        result = await self.graph.ainvoke(state)
        return result.get("final_response", "I couldn't process your request.")
'''
        
        template = Template(coordinator_content)
        content = template.render(settings=settings)
        
        coordinator_file = agents_dir / "coordinator.py"
        with open(coordinator_file, "w") as f:
            f.write(content)
        generated_files.append(str(coordinator_file))
        
        # Generate specialized agents based on architecture
        for agent_config in architecture.get("agents", []):
            if agent_config["role"] != "coordinator":
                agent_file = self._generate_specialized_agent(agent_config, agents_dir)
                generated_files.append(agent_file)
        
        return generated_files
    
    def _generate_specialized_agent(self, agent_config: Dict[str, Any], agents_dir: Path) -> str:
        """Generate a specialized agent"""
        
        agent_content = '''"""
{{ agent_config.name }} - {{ agent_config.description }}
"""
from typing import Dict, Any
from langchain_groq import ChatGroq

class {{ agent_config.name.title().replace('_', '') }}Agent:
    """{{ agent_config.description }}"""
    
    def __init__(self):
        self.llm = ChatGroq(
            groq_api_key=os.getenv("GROQ_API_KEY"),
            model_name="{{ agent_config.model }}",
            temperature={{ agent_config.temperature }}
        )
    
    async def process(self, message: str, context: Dict[str, Any] = None) -> str:
        """Process message with specialized capabilities"""
        
        system_prompt = f"""You are a {{ agent_config.role }} agent.
        
Your role: {{ agent_config.description }}
Your capabilities: {{ agent_config.capabilities | join(', ') }}
Available tools: {{ agent_config.tools | join(', ') }}

Focus on your specialized role and provide expert assistance."""

        response = await self.llm.ainvoke([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": message}
        ])
        
        return response.content
'''
        
        template = Template(agent_content)
        content = template.render(agent_config=agent_config)
        
        agent_file = agents_dir / f"{agent_config['name']}.py"
        with open(agent_file, "w") as f:
            f.write(content)
        
        return str(agent_file)
    
    def _generate_ui_components(self, config: ChatbotConfig, output_dir: Path) -> List[str]:
        """Generate UI components"""
        generated_files = []
        
        # Create UI directory
        ui_dir = output_dir / "ui"
        ui_dir.mkdir(exist_ok=True)
        
        # Generate custom CSS if provided
        if config.custom_css:
            css_file = ui_dir / "custom.css"
            with open(css_file, "w") as f:
                f.write(config.custom_css)
            generated_files.append(str(css_file))
        
        # Generate UI configuration
        ui_config_content = f'''"""
UI Configuration for {config.name}
"""

UI_CONFIG = {{
    "title": "{config.name}",
    "description": "{config.description}",
    "theme": "{config.ui_theme}",
    "port": {config.port},
    "logo_url": "{config.logo_url or ''}",
    "custom_css": "{config.custom_css or ''}",
    "max_conversation_length": {config.max_conversation_length}
}}
'''
        
        ui_config_file = ui_dir / "config.py"
        with open(ui_config_file, "w") as f:
            f.write(ui_config_content)
        generated_files.append(str(ui_config_file))
        
        return generated_files
    
    def _generate_config_files(self, config: ChatbotConfig, output_dir: Path) -> List[str]:
        """Generate configuration files"""
        generated_files = []
        
        # Generate .env template
        env_content = f'''# Environment Configuration for {config.name}
GROQ_API_KEY=your_groq_api_key_here

# Observability (optional)
OPIK_API_KEY=your_opik_api_key
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key

# Database
DATABASE_URL=sqlite:///./chatbot.db

# Configuration
MODEL_NAME={settings.default_model}
TEMPERATURE={settings.temperature}
MAX_TOKENS={settings.max_tokens}
'''
        
        env_file = output_dir / ".env.example"
        with open(env_file, "w") as f:
            f.write(env_content)
        generated_files.append(str(env_file))
        
        # Generate config.json
        config_json = {
            "name": config.name,
            "description": config.description,
            "type": config.chatbot_type,
            "capabilities": {
                "rag": config.enable_rag,
                "function_calling": config.enable_function_calling,
                "memory": config.enable_memory,
                "web_search": config.enable_web_search
            },
            "ui": {
                "theme": config.ui_theme,
                "port": config.port
            }
        }
        
        config_file = output_dir / "config.json"
        with open(config_file, "w") as f:
            json.dump(config_json, f, indent=2)
        generated_files.append(str(config_file))
        
        return generated_files
    
    def _generate_docker_files(self, config: ChatbotConfig, output_dir: Path) -> List[str]:
        """Generate Docker configuration files"""
        generated_files = []
        
        # Generate Dockerfile
        dockerfile_content = f'''FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    build-essential \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE {config.port}

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\
    CMD curl -f http://localhost:{config.port}/health || exit 1

# Run the application
CMD ["python", "main.py"]
'''
        
        dockerfile = output_dir / "Dockerfile"
        with open(dockerfile, "w") as f:
            f.write(dockerfile_content)
        generated_files.append(str(dockerfile))
        
        # Generate docker-compose.yml
        compose_content = f'''version: '3.8'

services:
  {config.name.lower().replace(' ', '-')}:
    build: .
    ports:
      - "{config.port}:{config.port}"
    environment:
      - GROQ_API_KEY=${{GROQ_API_KEY}}
      - OPIK_API_KEY=${{OPIK_API_KEY}}
      - LANGFUSE_SECRET_KEY=${{LANGFUSE_SECRET_KEY}}
      - LANGFUSE_PUBLIC_KEY=${{LANGFUSE_PUBLIC_KEY}}
    volumes:
      - ./data:/app/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:{config.port}/health"]
      interval: 30s
      timeout: 10s
      retries: 3
'''
        
        compose_file = output_dir / "docker-compose.yml"
        with open(compose_file, "w") as f:
            f.write(compose_content)
        generated_files.append(str(compose_file))
        
        return generated_files
    
    def _generate_setup_files(self, config: ChatbotConfig, architecture: Dict[str, Any], output_dir: Path) -> List[str]:
        """Generate setup and requirements files"""
        generated_files = []
        
        # Generate requirements.txt
        requirements = [
            "langgraph>=0.2.16",
            "langchain>=0.3.0",
            "langchain-groq>=0.2.0",
            "langchain-community>=0.3.0",
            "gradio>=4.44.0",
            "fastapi>=0.115.0",
            "uvicorn>=0.30.6",
            "python-dotenv>=1.0.1",
            "pydantic>=2.9.0"
        ]
        
        if config.enable_rag:
            requirements.extend([
                "chromadb>=0.5.5",
                "sentence-transformers>=2.2.2"
            ])
        
        if settings.opik_api_key:
            requirements.append("opik>=0.2.12")
        
        if settings.langfuse_secret_key:
            requirements.append("langfuse>=2.50.0")
        
        requirements_file = output_dir / "requirements.txt"
        with open(requirements_file, "w") as f:
            f.write("\n".join(requirements))
        generated_files.append(str(requirements_file))
        
        # Generate README.md
        readme_content = f'''# {config.name}

{config.description}

## Features

- **Type**: {config.chatbot_type}
- **Personality**: {", ".join(config.personality_traits)}
- **Capabilities**: 
  - RAG: {"✅" if config.enable_rag else "❌"}
  - Function Calling: {"✅" if config.enable_function_calling else "❌"}
  - Memory: {"✅" if config.enable_memory else "❌"}
  - Web Search: {"✅" if config.enable_web_search else "❌"}

## Quick Start

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Set up environment variables:
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

3. Run the chatbot:
   ```bash
   python main.py
   ```

4. Open your browser to `http://localhost:{config.port}`

## Docker Deployment

```bash
docker-compose up -d
```

## API Usage

```python
import requests

response = requests.post("http://localhost:{config.port}/chat", 
                        json={{"message": "Hello!", "user_id": "user123"}})
print(response.json())
```

## Generated by Chatbot Factory

This chatbot was automatically generated using the Chatbot Factory system.
'''
        
        readme_file = output_dir / "README.md"
        with open(readme_file, "w") as f:
            f.write(readme_content)
        generated_files.append(str(readme_file))
        
        return generated_files
